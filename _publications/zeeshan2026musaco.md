---
layout: publication
title: MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training
bibtex_key: zeeshan2026musaco
---

## Leveraging Multimodality for Personalized Expression Recognition

**Published in *WACV 2026: IEEE Winter Conf. on Applications of Computer Vision***

---

## ðŸ” Problem Overview

[cite_start]Personalized expression recognition (ER) involves adapting machine learning models to subject-specific data to improve recognition of expressions with considerable inter-personal variability[cite: 9]. [cite_start]While Multi-Source Domain Adaptation (MSDA) can improve robustness by treating subjects as domains, current state-of-the-art methods often overlook multimodal information or blend sources into a single domain[cite: 10, 11]. [cite_start]This limits subject diversity and fails to explicitly capture unique subject-specific characteristics[cite: 11].

{% include figure.liquid loading="eager" path="assets/img/publications/musaco_motiv.png" class="img-fluid rounded z-depth-1" %}

[cite_start]**Figure 1** compares different approaches[cite: 66]:

* [cite_start]**(a) Unimodal MSDA:** Aligns sources within a single modality, which often reduces accuracy[cite: 67].
* [cite_start]**(b) Multimodal UDA (Blending):** Blends sources into a single domain, failing to exploit subject-specific diversity[cite: 68].
* [cite_start]**(c) MuSACO (Ours):** Selects relevant sources per modality using co-training and aligns them using both class-aware and class-agnostic losses[cite: 69].

> [cite_start]**Existing methods often fail to generalize for subtle expressions and across diverse individuals due to variations in cultural and individual expressiveness[cite: 23].**

---

<!-- ## ðŸ’¡ Our Proposed Method: MuSACO

[cite_start]We introduce **MuSACO**, a Multimodal Subject-Specific Selection and Adaptation method based on Co-Training[cite: 12]. [cite_start]It leverages complementary information across modalities (e.g., visual and physiological) and selects relevant source subjects for adaptation[cite: 13].

The method consists of three key stages:

---

## 1ï¸âƒ£ **Source Selection via Co-Training**

[cite_start]To address the challenge of leveraging multiple source subjects, MuSACO selects sources most relevant to the target[cite: 80].

* [cite_start]We estimate similarity scores between source and target subjects using embeddings from each modality[cite: 210].
* [cite_start]Using a co-training strategy, sources that produce high similarity scores in either modality are selected[cite: 131].
* [cite_start]A threshold is applied to filter out less relevant sources, ensuring only the most informative subjects are used[cite: 131].

<!-- {% include figure.liquid loading="eager" path="assets/img/publications/musaco_fig_3.png" class="img-fluid rounded z-depth-1" %}

[cite_start]*Visualization of selected (green) and non-selected (red) source subjects based on similarity scores[cite: 346].* -->

---

## 2ï¸âƒ£ **Dual-Loss Domain Alignment**

[cite_start]MuSACO employs a robust alignment process using target Pseudo-Labels (PLs)[cite: 89]:

#### âœ” Generating Target Pseudo-Labels (PLs)
[cite_start]PLs are generated by selecting predictions from the dominant modality (e.g., visual or physiological) based on probability scores[cite: 90]. [cite_start]This ensures diversity in feature representations[cite: 91].

#### âœ” Class-Aware & Class-Agnostic Alignment
* [cite_start]**Class-Aware Loss:** We use confident target samples (selected via threshold) to minimize distribution mismatch through class-aware alignment[cite: 92].
* [cite_start]**Class-Agnostic Loss:** To utilize useful but less confident samples (which are often discarded), we introduce a class-agnostic loss that aligns non-confident target samples with the source[cite: 96, 97].

---

## 3ï¸âƒ£ **Disentanglement & Fusion**

To improve generalization:

* [cite_start]**Disentanglement:** We use an entropy-based estimator (KNIFE) to disentangle identity-related information from expression-specific features[cite: 200, 203].
* [cite_start]**Modality Alignment:** Features from different modalities are concatenated for each selected source and confident target subject, then processed through a fusion module[cite: 263, 265].

{% include figure.liquid loading="eager" path="assets/img/publications/musaco.png" class="img-fluid rounded z-depth-1" %}

[cite_start]*Overview of the MuSACO architecture, illustrating the co-training loop, source selection, and alignment modules[cite: 186].*

---

## ðŸ“ˆ Results

[cite_start]We evaluated MuSACO on three multimodal datasets: **BioVid** (pain), **StressID** (stress), and **BAH** (ambivalence/hesitancy)[cite: 17].

**Key Findings:**

* [cite_start]**BioVid:** MuSACO achieved **43.8% accuracy**, outperforming Unimodal MSDA (34.7%) and Blended UDA (36.3%)[cite: 306, 311].
* [cite_start]**StressID:** Achieved an overall gain of **15.7%** over the lower bound and consistently outperformed state-of-the-art MSDA methods[cite: 352].
* [cite_start]**BAH:** In uncontrolled, real-world settings, MuSACO surpassed all baselines, achieving **41.2% Average F1**[cite: 358].

> [cite_start]**MuSACO consistently outperforms UDA (blending) and state-of-the-art MSDA methods on challenging multimodal data[cite: 17].**

---

## âœ¨ Takeaway

[cite_start]MuSACO supports personalized modeling by adapting to each target subject through relevant sources[cite: 398]. [cite_start]This makes it particularly relevant for digital health applications, such as patient-specific assessment for stress or pain[cite: 14].

---

## ðŸ“„ Full Paper

**Title:** MuSACO: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training
**Authors:** Muhammad Osama Zeeshan, Natacha Gillet, Alessandro Lameiras Koerich, Marco Pedersoli, Francois Bremond, Eric Granger
**Venue:** WACV 2026: IEEE Winter Conf. on Applications of Computer Vision, Tucson, Arizona, USA

ðŸ”— **Read the paper:** [arXiv Link](https://arxiv.org/abs/2508.12522v2)
[cite_start]ðŸ’» **Code:** [GitHub MuSACO](https://github.com/osamazeeshan/MuSACo) [cite: 18]

---

## ðŸ“¬ Contact

For questions regarding the implementation or methodology, please contact the authors at Ã‰TS Montreal or Inria. -->