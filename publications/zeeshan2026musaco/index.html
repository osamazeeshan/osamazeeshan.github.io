<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="leveraging-multimodality-for-personalized-expression-recognition">Leveraging Multimodality for Personalized Expression Recognition</h2> <p><strong>Published in <em>WACV 2026: IEEE Winter Conf. on Applications of Computer Vision</em></strong></p> <hr> <h2 id="-problem-overview">üîç Problem Overview</h2> <p>[cite_start]Personalized expression recognition (ER) involves adapting machine learning models to subject-specific data to improve recognition of expressions with considerable inter-personal variability[cite: 9]. [cite_start]While Multi-Source Domain Adaptation (MSDA) can improve robustness by treating subjects as domains, current state-of-the-art methods often overlook multimodal information or blend sources into a single domain[cite: 10, 11]. [cite_start]This limits subject diversity and fails to explicitly capture unique subject-specific characteristics[cite: 11].</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/musaco_motiv-480.webp 480w,/assets/img/publications/musaco_motiv-800.webp 800w,/assets/img/publications/musaco_motiv-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publications/musaco_motiv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>[cite_start]<strong>Figure 1</strong> compares different approaches[cite: 66]:</p> <ul> <li>[cite_start]<strong>(a) Unimodal MSDA:</strong> Aligns sources within a single modality, which often reduces accuracy[cite: 67].</li> <li>[cite_start]<strong>(b) Multimodal UDA (Blending):</strong> Blends sources into a single domain, failing to exploit subject-specific diversity[cite: 68].</li> <li>[cite_start]<strong>(c) MuSACO (Ours):</strong> Selects relevant sources per modality using co-training and aligns them using both class-aware and class-agnostic losses[cite: 69].</li> </ul> <blockquote> <p>[cite_start]<strong>Existing methods often fail to generalize for subtle expressions and across diverse individuals due to variations in cultural and individual expressiveness[cite: 23].</strong></p> </blockquote> <hr> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!--
  See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
  https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
--&gt;

  &lt;source
    class="responsive-img-srcset"
    srcset="/assets/img/publications/musaco_fig_3-480.webp 480w,/assets/img/publications/musaco_fig_3-800.webp 800w,/assets/img/publications/musaco_fig_3-1400.webp 1400w,"
    
      sizes="95vw"
    
    type="image/webp"
  &gt;

&lt;img
  src="/assets/img/publications/musaco_fig_3.png"
  
    class="img-fluid rounded z-depth-1"
  
  
    width="100%"
  
  
    height="auto"
  
  
  
  
  
  
    loading="eager"
  
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <p>[cite_start]<em>Visualization of selected (green) and non-selected (red) source subjects based on similarity scores[cite: 346].</em> ‚Äì&gt;</p> <hr> <h2 id="2Ô∏è‚É£-dual-loss-domain-alignment">2Ô∏è‚É£ <strong>Dual-Loss Domain Alignment</strong> </h2> <p>[cite_start]MuSACO employs a robust alignment process using target Pseudo-Labels (PLs)[cite: 89]:</p> <h4 id="-generating-target-pseudo-labels-pls">‚úî Generating Target Pseudo-Labels (PLs)</h4> <p>[cite_start]PLs are generated by selecting predictions from the dominant modality (e.g., visual or physiological) based on probability scores[cite: 90]. [cite_start]This ensures diversity in feature representations[cite: 91].</p> <h4 id="-class-aware--class-agnostic-alignment">‚úî Class-Aware &amp; Class-Agnostic Alignment</h4> <ul> <li>[cite_start]<strong>Class-Aware Loss:</strong> We use confident target samples (selected via threshold) to minimize distribution mismatch through class-aware alignment[cite: 92].</li> <li>[cite_start]<strong>Class-Agnostic Loss:</strong> To utilize useful but less confident samples (which are often discarded), we introduce a class-agnostic loss that aligns non-confident target samples with the source[cite: 96, 97].</li> </ul> <hr> <h2 id="3Ô∏è‚É£-disentanglement--fusion">3Ô∏è‚É£ <strong>Disentanglement &amp; Fusion</strong> </h2> <p>To improve generalization:</p> <ul> <li>[cite_start]<strong>Disentanglement:</strong> We use an entropy-based estimator (KNIFE) to disentangle identity-related information from expression-specific features[cite: 200, 203].</li> <li>[cite_start]<strong>Modality Alignment:</strong> Features from different modalities are concatenated for each selected source and confident target subject, then processed through a fusion module[cite: 263, 265].</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/musaco-480.webp 480w,/assets/img/publications/musaco-800.webp 800w,/assets/img/publications/musaco-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publications/musaco.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>[cite_start]<em>Overview of the MuSACO architecture, illustrating the co-training loop, source selection, and alignment modules[cite: 186].</em></p> <hr> <h2 id="-results">üìà Results</h2> <p>[cite_start]We evaluated MuSACO on three multimodal datasets: <strong>BioVid</strong> (pain), <strong>StressID</strong> (stress), and <strong>BAH</strong> (ambivalence/hesitancy)[cite: 17].</p> <p><strong>Key Findings:</strong></p> <ul> <li>[cite_start]<strong>BioVid:</strong> MuSACO achieved <strong>43.8% accuracy</strong>, outperforming Unimodal MSDA (34.7%) and Blended UDA (36.3%)[cite: 306, 311].</li> <li>[cite_start]<strong>StressID:</strong> Achieved an overall gain of <strong>15.7%</strong> over the lower bound and consistently outperformed state-of-the-art MSDA methods[cite: 352].</li> <li>[cite_start]<strong>BAH:</strong> In uncontrolled, real-world settings, MuSACO surpassed all baselines, achieving <strong>41.2% Average F1</strong>[cite: 358].</li> </ul> <blockquote> <p>[cite_start]<strong>MuSACO consistently outperforms UDA (blending) and state-of-the-art MSDA methods on challenging multimodal data[cite: 17].</strong></p> </blockquote> <hr> <h2 id="-takeaway">‚ú® Takeaway</h2> <p>[cite_start]MuSACO supports personalized modeling by adapting to each target subject through relevant sources[cite: 398]. [cite_start]This makes it particularly relevant for digital health applications, such as patient-specific assessment for stress or pain[cite: 14].</p> <hr> <h2 id="-full-paper">üìÑ Full Paper</h2> <p><strong>Title:</strong> MuSACO: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training <strong>Authors:</strong> Muhammad Osama Zeeshan, Natacha Gillet, Alessandro Lameiras Koerich, Marco Pedersoli, Francois Bremond, Eric Granger <strong>Venue:</strong> WACV 2026: IEEE Winter Conf. on Applications of Computer Vision, Tucson, Arizona, USA</p> <p>üîó <strong>Read the paper:</strong> <a href="https://arxiv.org/abs/2508.12522v2" rel="external nofollow noopener" target="_blank">arXiv Link</a> [cite_start]üíª <strong>Code:</strong> <a href="https://github.com/osamazeeshan/MuSACo" rel="external nofollow noopener" target="_blank">GitHub MuSACO</a> [cite: 18]</p> <hr> <h2 id="-contact">üì¨ Contact</h2> <p>For questions regarding the implementation or methodology, please contact the authors at √âTS Montreal or Inria. ‚Äì&gt;</p> </body></html>