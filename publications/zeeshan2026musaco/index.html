<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="leveraging-multimodality-for-personalized-expression-recognition">Leveraging Multimodality for Personalized Expression Recognition</h2> <p><strong>Published in <em>WACV 2026: IEEE Winter Conf. on Applications of Computer Vision</em></strong></p> <hr> <h2 id="-problem-overview">üîç Problem Overview</h2> <p>Personalized expression recognition (ER) involves adapting machine learning models to subject-specific data to improve recognition of expressions with considerable inter-personal variability. While Multi-Source Domain Adaptation (MSDA) can improve robustness by treating subjects as domains, current state-of-the-art methods often overlook multimodal information or blend sources into a single domain. This limits subject diversity and fails to explicitly capture unique subject-specific characteristics.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/musaco_motiv-480.webp 480w,/assets/img/publications/musaco_motiv-800.webp 800w,/assets/img/publications/musaco_motiv-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publications/musaco_motiv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 1</strong> compares different approaches:</p> <p><strong>(a) Unimodal MSDA:</strong> Aligns sources within a single modality, which often reduces accuracy. <strong>(b) Multimodal UDA (Blending):</strong> Blends sources into a single domain, failing to exploit subject-specific diversity. <strong>(c) MuSACO (Ours):</strong> Selects relevant sources per modality using co-training and aligns them using both class-aware and class-agnostic losses.</p> <p><strong>Existing methods often fail to generalize for subtle expressions and across diverse individuals due to variations in cultural and individual expressiveness.</strong></p> <hr> <h2 id="-our-proposed-method-musaco">üí° Our Proposed Method: MuSACO</h2> <p>We introduce <strong>MuSACO</strong>, a Multimodal Subject-Specific Selection and Adaptation method based on Co-Training. It leverages complementary information across modalities (e.g., visual and physiological) and selects relevant source subjects for adaptation.</p> <p>The method consists of three key stages:</p> <hr> <h2 id="1Ô∏è‚É£-source-selection-via-co-training">1Ô∏è‚É£ <strong>Source Selection via Co-Training</strong> </h2> <p>To address the challenge of leveraging multiple source subjects, MuSACO selects sources most relevant to the target.</p> <ul> <li>We estimate similarity scores between source and target subjects using embeddings from each modality.</li> <li>Using a co-training strategy, sources that produce high similarity scores in either modality are selected.</li> <li>A threshold is applied to filter out less relevant sources, ensuring only the most informative subjects are used.</li> </ul> <hr> <h2 id="2Ô∏è‚É£-dual-loss-domain-alignment">2Ô∏è‚É£ <strong>Dual-Loss Domain Alignment</strong> </h2> <p>MuSACO employs a robust alignment process using target Pseudo-Labels (PLs):</p> <h4 id="-generating-target-pseudo-labels-pls">‚úî Generating Target Pseudo-Labels (PLs)</h4> <p>PLs are generated by selecting predictions from the dominant modality (e.g., visual or physiological) based on probability scores. This ensures diversity in feature representations.</p> <h4 id="-class-aware--class-agnostic-alignment">‚úî Class-Aware &amp; Class-Agnostic Alignment</h4> <ul> <li> <strong>Class-Aware Loss:</strong> We use confident target samples (selected via threshold) to minimize distribution mismatch through class-aware alignment.</li> <li> <strong>Class-Agnostic Loss:</strong> To utilize useful but less confident samples (which are often discarded), we introduce a class-agnostic loss that aligns non-confident target samples with the source.</li> </ul> <hr> <h2 id="3Ô∏è‚É£-disentanglement--fusion">3Ô∏è‚É£ <strong>Disentanglement &amp; Fusion</strong> </h2> <p>To improve generalization:</p> <p><strong>Disentanglement:</strong> We use an entropy-based estimator (KNIFE) to disentangle identity-related information from expression-specific features. <strong>Modality Alignment:</strong> Features from different modalities are concatenated for each selected source and confident target subject, then processed through a fusion module.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/musaco-480.webp 480w,/assets/img/publications/musaco-800.webp 800w,/assets/img/publications/musaco-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publications/musaco.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><em>Overview of the MuSACO architecture, illustrating the co-training loop, source selection, and alignment modules.</em></p> <hr> <h2 id="-results">üìà Results</h2> <p>We evaluated MuSACO on three multimodal datasets: <strong>BioVid</strong> (pain), <strong>StressID</strong> (stress), and <strong>BAH</strong> (ambivalence/hesitancy).</p> <p><strong>Key Findings:</strong></p> <ul> <li> <strong>BioVid:</strong> MuSACO achieved <strong>43.8% accuracy</strong>, outperforming Unimodal MSDA (34.7%) and Blended UDA (36.3%).</li> <li> <strong>StressID:</strong> Achieved an overall gain of <strong>15.7%</strong> over the lower bound and consistently outperformed state-of-the-art MSDA methods.</li> <li> <strong>BAH:</strong> In uncontrolled, real-world settings, MuSACO surpassed all baselines, achieving <strong>41.2% Average F1</strong>.</li> </ul> <blockquote> <p><strong>MuSACO consistently outperforms UDA (blending) and state-of-the-art MSDA methods on challenging multimodal data.</strong></p> </blockquote> <hr> <h2 id="-takeaway">‚ú® Takeaway</h2> <p>MuSACO supports personalized modeling by adapting to each target subject through relevant sources. This makes it particularly relevant for digital health applications, such as patient-specific assessment for stress or pain.</p> <hr> <h2 id="-full-paper">üìÑ Full Paper</h2> <p><strong>Title:</strong> MuSACO: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training <strong>Authors:</strong> Muhammad Osama Zeeshan, Natacha Gillet, Alessandro Lameiras Koerich, Marco Pedersoli, Francois Bremond, Eric Granger <strong>Venue:</strong> WACV 2026: IEEE Winter Conf. on Applications of Computer Vision, Tucson, Arizona, USA</p> <p>üîó <strong>Read the paper:</strong> <a href="https://arxiv.org/abs/2508.12522v2" rel="external nofollow noopener" target="_blank">arXiv Link</a> üíª <strong>Code:</strong> <a href="https://github.com/osamazeeshan/MuSACo" rel="external nofollow noopener" target="_blank">GitHub MuSACO</a></p> <hr> <h2 id="-contact">üì¨ Contact</h2> <p>For questions regarding the implementation or methodology, please contact the authors at √âTS Montreal or Inria.</p> </body></html>