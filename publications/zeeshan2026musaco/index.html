<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MuSACo - Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training | Muhammad Osama Zeeshan </title> <meta name="author" content="Muhammad Osama Zeeshan"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%94&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://osamazeeshan.github.io/publications/zeeshan2026musaco/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Muhammad</span> Osama Zeeshan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Knowledge Sharing </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MuSACo - Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training</h1> </header> <article> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <a href="/publications/zeeshan2026musaco/" style="text-decoration: none; display: block; cursor: pointer; opacity: 1; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.8'" onmouseout="this.style.opacity='1'"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/musaco-480.webp 480w,/assets/img/publication_preview/musaco-800.webp 800w,/assets/img/publication_preview/musaco-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/musaco.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="musaco.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </a> </div> <div id="zeeshan2026musaco" class="col-sm-8"> <div class="title"><a href="/publications/zeeshan2026musaco/" style="text-decoration: none; color: inherit; cursor: pointer; transition: color 0.2s;" onmouseover="this.style.color='var(--global-theme-color, #007bff)'" onmouseout="this.style.color='inherit'">MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training</a></div> <div class="author"> Muhammad Osama Zeeshan ,¬†Natacha Gillet ,¬†Alessandro Lameiras Koerich , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Marco Pedersoli, Francois Bremond, Eric Granger' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>WACV 2026: IEEE Winter Conf. on Applications of Computer Vision, Tucson, Arizona, USA</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> <hr> <div class="publication-content"> <h2 id="leveraging-multimodality-for-personalized-expression-recognition">Leveraging Multimodality for Personalized Expression Recognition</h2> <p><strong>Published in WACV 2026: IEEE Winter Conf. on Applications of Computer Vision</strong></p> <hr> <h2 id="-problem-overview">üîç Problem Overview</h2> <p>Personalized expression recognition (ER) involves adapting machine learning models to subject-specific data to improve recognition of expressions with considerable inter-personal variability. While Multi-Source Domain Adaptation (MSDA) can improve robustness by treating subjects as domains, current state-of-the-art methods often overlook multimodal information or blend sources into a single domain. This limits subject diversity and fails to explicitly capture unique subject-specific characteristics.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/musaco_motiv-480.webp 480w,/assets/img/publications/musaco_motiv-800.webp 800w,/assets/img/publications/musaco_motiv-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publications/musaco_motiv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 1</strong> compares different approaches:</p> <ul> <li> <p>(a) Unimodal MSDA: Aligns sources within a single modality, which often reduces accuracy.</p> </li> <li> <p>(b) Multimodal UDA (Blending): Blends sources into a single domain, failing to exploit subject-specific diversity.</p> </li> <li> <p>(c) MuSACO (Ours): Selects relevant sources per modality using co-training and aligns them using both class-aware and class-agnostic losses.</p> </li> </ul> <p><strong>Existing methods often fail to generalize for subtle expressions and across diverse individuals due to variations in cultural and individual expressiveness.</strong></p> <hr> <h2 id="-our-proposed-method-musaco">üí° Our Proposed Method: MuSACO</h2> <p>We introduce <strong>MuSACO</strong>, a Multimodal Subject-Specific Selection and Adaptation method based on Co-Training. It leverages complementary information across modalities (e.g., visual and physiological) and selects relevant source subjects for adaptation.</p> <p>The method consists of three key stages:</p> <hr> <h2 id="1Ô∏è‚É£-source-selection-via-co-training">1Ô∏è‚É£ <strong>Source Selection via Co-Training</strong> </h2> <p>To address the challenge of leveraging multiple source subjects, MuSACO selects sources most relevant to the target.</p> <ul> <li>We estimate similarity scores between source and target subjects using embeddings from each modality.</li> <li>Using a co-training strategy, sources that produce high similarity scores in either modality are selected.</li> <li>A threshold is applied to filter out less relevant sources, ensuring only the most informative subjects are used.</li> </ul> <hr> <h2 id="2Ô∏è‚É£-dual-loss-domain-alignment">2Ô∏è‚É£ <strong>Dual-Loss Domain Alignment</strong> </h2> <p>MuSACO employs a robust alignment process using target Pseudo-Labels (PLs):</p> <h4 id="-generating-target-pseudo-labels-pls">‚úî Generating Target Pseudo-Labels (PLs)</h4> <p>PLs are generated by selecting predictions from the dominant modality (e.g., visual or physiological) based on probability scores. This ensures diversity in feature representations.</p> <h4 id="-class-aware--class-agnostic-alignment">‚úî Class-Aware &amp; Class-Agnostic Alignment</h4> <ul> <li> <strong>Class-Aware Loss:</strong> We use confident target samples (selected via threshold) to minimize distribution mismatch through class-aware alignment.</li> <li> <strong>Class-Agnostic Loss:</strong> To utilize useful but less confident samples (which are often discarded), we introduce a class-agnostic loss that aligns non-confident target samples with the source.</li> </ul> <hr> <h2 id="3Ô∏è‚É£-disentanglement--fusion">3Ô∏è‚É£ <strong>Disentanglement &amp; Fusion</strong> </h2> <p>To improve generalization:</p> <p><strong>Disentanglement:</strong> We use an entropy-based estimator (KNIFE) to disentangle identity-related information from expression-specific features. <strong>Modality Alignment:</strong> Features from different modalities are concatenated for each selected source and confident target subject, then processed through a fusion module.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/musaco-480.webp 480w,/assets/img/publications/musaco-800.webp 800w,/assets/img/publications/musaco-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/publications/musaco.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><em>Overview of the MuSACO architecture, illustrating the co-training loop, source selection, and alignment modules.</em></p> <hr> <h2 id="-results">üìà Results</h2> <p>We evaluated MuSACO on three multimodal datasets: <strong>BioVid</strong> (pain), <strong>StressID</strong> (stress), and <strong>BAH</strong> (ambivalence/hesitancy).</p> <p><strong>Key Findings:</strong></p> <ul> <li> <strong>BioVid:</strong> MuSACO achieved <strong>43.8% accuracy</strong>, outperforming Unimodal MSDA (34.7%) and Blended UDA (36.3%).</li> <li> <strong>StressID:</strong> Achieved an overall gain of <strong>15.7%</strong> over the lower bound and consistently outperformed state-of-the-art MSDA methods.</li> <li> <strong>BAH:</strong> In uncontrolled, real-world settings, MuSACO surpassed all baselines, achieving <strong>41.2% Average F1</strong>.</li> </ul> <blockquote> <p><strong>MuSACO consistently outperforms UDA (blending) and state-of-the-art MSDA methods on challenging multimodal data.</strong></p> </blockquote> <hr> <h2 id="-takeaway">‚ú® Takeaway</h2> <p>MuSACO supports personalized modeling by adapting to each target subject through relevant sources. This makes it particularly relevant for digital health applications, such as patient-specific assessment for stress or pain.</p> <hr> <h2 id="-full-paper">üìÑ Full Paper</h2> <p><strong>Title:</strong> MuSACO: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training <strong>Authors:</strong> Muhammad Osama Zeeshan, Natacha Gillet, Alessandro Lameiras Koerich, Marco Pedersoli, Francois Bremond, Eric Granger <strong>Venue:</strong> WACV 2026: IEEE Winter Conf. on Applications of Computer Vision, Tucson, Arizona, USA</p> <p>üîó <strong>Read the paper:</strong> <a href="https://arxiv.org/abs/2508.12522v2" rel="external nofollow noopener" target="_blank">arXiv Link</a> üíª <strong>Code:</strong> <a href="https://github.com/osamazeeshan/MuSACo" rel="external nofollow noopener" target="_blank">GitHub MuSACO</a></p> <hr> <h2 id="-contact">üì¨ Contact</h2> <p>For questions regarding the implementation or methodology, please contact the authors at √âTS Montreal or Inria.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Muhammad Osama Zeeshan. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>